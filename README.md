# TensorFlowGettingStarted
Here is an accompanying code portion of my <a href=https://medium.com/@TimWroge/getting-started-with-tensorflow-introduction-to-neural-networks-7ba9eeefc47f>Getting Started with TensorFlow blog on Medium </a>. 

This code should be a good stepping stone to legitimate coding with TensorFlow to construct advanced neural network architechtures on a low level, in order to understand the higher level API, when we get to that point. 


Here is a basic refresher on Neural Networks:

So the first question you may have is, why does a library like TensorFlow exist in the first place? TensorFlow is a library that makes solving machine learning problems easier. Broadly speaking, machine learning is a branch of computer science that makes machines learn based on data, not through rule based systems. Traditionally, coders were dealt the task of taking repetitive tasks and having computers complete them based on strict rules. This approach was great for sorting 32 bit integers, or even evaluating symbolic integrals. But how would someone generate a rule based system for how to differentiate between a cat and a dog? They may begin by listing all the different characteristics of a cat or a dog and generating a library of possible dogs and cats and search that library to determine whether or not a specific photo is determined to be a cat or dog. Even if that system were able to preform perfectly on an image classification task, what happens if you want to create a new system to classify cars? You would have to start from scratch. In this way, this rule based approach on certain problems is not scale-able. Many problems of computer science that, us, humans, can do without thinking become extraordinarily difficult to program traditionally, like speech recognition, image classification, spacial reasoning etc.
In this blog I will be covering one specific area of machine learning and artificial intelligence that has gained a good deal of traction in the past few years: neural networks. Neural networks are loosely based on human brain cells and how those brain cells (neurons) stack up for us to understand higher levels of complexity.

A biological neuron is composed of a nucleus, an axon and dendrites. The axon branches and connects to other neurons, while the dendritic tree receives signals from the axons of other neurons. Between these neurons, are synapses, which send signals between neurons. These simple connections of neurons are layered upon themselves to create the complexity of the human mind. On a very basic level, a neuron receives input from neighboring neurons and, based on their collective influence, the neuron fires down the axon or does not (depending on the specific threshold the neuron has). The higher the threshold, the more signals it takes to fire the neuron. Overtime, the connections between the neurons change, this is called synaptic plasticity. Based on the what is known as Hebb’s Law, what fires together wires together. Analogously, an artificial neuron within an artificial neural network (ANN) receives inputs from the other neurons it is connected to (simulated through input * weight) and sums their influences. A specific neuron may wish to respond more to some neurons rather than others. This is reflected in the weights of the synapses (see below) between the different neurons.

To use the image from Vinícius Gonçalves Maltarollo’s Paper, B represents the summing the influence over individual neurons (weight_i * x_i for 1≤i ≤n) across the synapses and applying an activation function. This represents the “collective influence ” over all connected neurons. C and D show how the biological synapses are modeled by lines connecting neurons. Here another illustration of what is going on over individual synapses in an artificial neuron:


This simply means that every neuron takes the sum of the weights times the corresponding input and adds a bias. In this way, applying the operation of forward propagation (fancy way of saying the input flowing through the ANN), is really just a linear function applied on the input! Just like you might have seen in a high school geometry class: y=m*x+b. The difference between a simple classifier like a straight line, and a neural network, is that a ANN stacks these linear approximators on top of one another and applies what is called an activation function on each. An activation function takes a certain input (in this case, the sum of the weights* input +bias)and changes it in such a way so that it can fit non-linear data sets. This is analogous to how a biological neural network responds to specific inputs from other neurons. The network sums all the inputs times the weights, plus the bias and applies an activation function to that sum. Here are some common activation functions:

These activation functions are critical in evaluating non-linear inputs relationships, and certain functions (like the logistic sigmoid) will be crucial when we will approach recurrent neural networks.
How do you code something like this ?
For the sake of clarity, I will make an example of a certain problem we wish to answer using a neural network. For instance, what will the weather be today, given the humidity, temperature, latitude, and season? In order to parse this information in a way that a computer can understand it, we need to vector-ize it! A vector is just a way of expressing a one dimensional array of numbers (like [1, 2 , 5, 7] or [1 ; 2 ;6]).
In this example, we have a specific layout of the vector which will be the input to the neural network. This will be [Humidity; Temperature; Latitude; Season]. You may look at this and think that this could not possibly make any sense, how can a season be a number ? This is where “one-hot encoding” comes in: we can denote fall, winter, spring , summer as [1;0;0;0], [0;1;0;0], [0;0;1;0] and [0;0;0;1] respectively. “One-hot” simply means that the vector has all zeros, except one maximum value of 1 occurring at some instance, corresponding to some value. So in this example, a particular day may have a weather vector [.8;35; 41.2033; 0; 0; 0;1] meaning that it is a day with 80 % humidity, 35 degrees Celsius, at 41.2033 degrees north , and during summer.
Then we need to take that input and send that through the network.

Applying the weights of a given synapse corresponds to matrix multiplying the input (in this case the weather vector) and the weight matrix. This is an example of that product plus a bias:

The dimensions of the weight matrix correspond to(the number of neurons in the next layer) x (length of the vector). For the sake of simplicity, we will have a neural network that has one hidden layer (layer of neurons between input and output) of size 3 and an output layer that corresponds to the type of weather we want to classify: [sunny; rainy; cloudy] of size 3. Here, the H vector denotes the values of the hidden layer before the activation function is applied. And the O vector will denote the output of the neural network. So the mathematical operation of forward propagation would look something like this (excuse my reuse of variables B, Z and W):

One thing I added that I failed to mention earlier is something called the softmax regression. It is defined as follows:

This function squashes any array of inputs into a series of values between 0 and 1, that add to one. Now the ANN outputs probabilities ! So, during training (when the ANN learns the values it needs for application) negative outputs for rain would correspond to a low prediction probability for rain and a positive value for cloudy will correspond to a high prediction probability for a cloudy day.
So where are we at ? We made a way of propagating information about the current weather to predict what will happen later as probabilities. But if we made the code for this, then the output would just be random numbers between zero and one. That is where training the network comes in! The algorithm we use for this is something called back propagation (you can see it in action in the GIF above) using gradient descent. You may remember back in Calculus I when you had to find the minima and maxima of a function. Minima or maxima always occur when the slope is zero, so if you have a function of the curve in question, then by setting the derivative of that function to zero, you can find where those minima or maxima occur. We will use a similar approach here. This time, we will use the gradient. A gradient is the vector of partial derivatives of a given function. Here is what that would look like geometrically for a two-variable function:

As you can see, the gradient always points uphill for a given function. We will use the algorithm of gradient descent to minimize the cost function. A cost function is one that gives a value for how bad a given prediction is. For our weather example, we may have a weather vector that tells us what the weather is for a given input. I will denote this as the Y vector which will represent the “one hot” representation of if it is sunny, rainy or cloudy.
For instance, we may choose to use this this function:

One thing that is important to note about a cost function is that gradient descent will work best if the function in question is non-convex. If we are trying to reach a minima, then we have to be trying to reach the bottom of a sort of bowl-like surface in the weight space, so we minimize the error in our prediction. If this was not the case, then the algorithm will continue ad infinitum trying to minimize the function.
So how does the algorithm work? Basically, we are going to follow the opposite of the gradient to reach a valley in the cost function by changing the weights on the synapses. This is what that would look like:

Imagine what would happen if this error surface was a plane (like it might be if we simply took the difference of the prediction from the cost). The gradient would remain constant and always decreasing, basically getting nowhere. So how exactly does this optimize the network? That is where the chain rule comes in. Basically, we take small steps (determined by the learning rate) in the opposite direction of the gradient of the cost function, with respect to each weight value. Imagine if we took the derivative of the cost function above, with respect to a weight in one of the synapses of the original network. First, we can notice that the P vector is really a function of the activation functions, the weights and the input. Expanding it would look something like this:

Here, W_1 and B_1 indicates the weight matrix and the bias vector respectively for the first layer of the neural network and the W_2 and B_2 represent the second layer of synapses’ parameters. A traditional neural network would require computing the derivative for that function (within the cost) with respect to each bias, weight etc. I will not show all the steps but needless to say it is really annoying to do by hand.
One consequence of the chain rule is that the weights that contribute most to the error get changed the most and the ones that contribute the least do not get changed as much. Over many iterations, the cost function reaches a local or global minima and you now have a trained model for the training data! 

